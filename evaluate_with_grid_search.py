import torch
import numpy as np
from torch.utils.data import DataLoader
from src.dataset import MultiFileDataset, collate_pad
from src.model import SpotRNA_LSTM_Refined  # ç¡®ä¿è¿™é‡Œå¼•ç”¨çš„ç±»åå’Œä½ è®­ç»ƒæ—¶ä¸€è‡´
from src.config import Config
from tqdm import tqdm


# --- è´ªå¿ƒè§£ç å‡½æ•° (å¸¦é˜ˆå€¼) ---
def greedy_decoding(prob_map, threshold=0.3):
    seq_len = prob_map.shape[0]
    structure = np.zeros((seq_len, seq_len))
    visited = set()
    candidates = []

    # 1. ç­›é€‰å¤§äºé˜ˆå€¼çš„ç‚¹
    for i in range(seq_len):
        for j in range(i + 1, seq_len):
            if prob_map[i, j] > threshold:
                candidates.append((prob_map[i, j], i, j))

    # 2. æŒ‰æ¦‚ç‡ä»å¤§åˆ°å°æ’åº
    candidates.sort(key=lambda x: x[0], reverse=True)

    # 3. è´ªå¿ƒé€‰æ‹©ï¼ˆäº’æ–¥ï¼‰
    for prob, i, j in candidates:
        if i not in visited and j not in visited:
            structure[i, j] = 1
            structure[j, i] = 1
            visited.add(i)
            visited.add(j)

    return structure


# --- æ ¸å¿ƒï¼šç½‘æ ¼æœç´¢è¯„ä¼°å‡½æ•° ---
def evaluate_with_grid_search(models, dataloader, device):
    for m in models: m.eval()

    # è¿™é‡Œè®¾å®šæˆ‘ä»¬è¦æ‰«æçš„é˜ˆå€¼èŒƒå›´
    # æ ¹æ®ä½ çš„è¶‹åŠ¿ï¼Œé‡ç‚¹æ‰«æ 0.25 - 0.35 åŒºåŸŸ
    thresholds = [0.20, 0.25, 0.30, 0.35, 0.40]

    # åˆå§‹åŒ–ç»Ÿè®¡å™¨ï¼šè®°å½•æ¯ä¸ªé˜ˆå€¼çš„æ€» P, R, F1
    # è¿™é‡Œçš„ F1 é‡‡ç”¨ Macro-Average (å…ˆç®—æ¯ä¸ªæ ·æœ¬çš„F1ï¼Œå†æ±‚å¹³å‡)ï¼Œå’Œä½ ä¹‹å‰çš„æŒ‡æ ‡ä¸€è‡´
    metrics = {t: {'f1_sum': 0, 'p_sum': 0, 'r_sum': 0} for t in thresholds}
    count = 0

    print(f"ğŸš€ å¼€å§‹ç½‘æ ¼æœç´¢ï¼Œæµ‹è¯•é˜ˆå€¼: {thresholds} ...")

    with torch.no_grad():
        for seqs, labels, masks in tqdm(dataloader):
            seqs = seqs.to(device)
            # labels ä¸ä¸Š GPU èŠ‚çœæ˜¾å­˜

            # 1. æ¨¡å‹é›†æˆæ¨ç† (æœ€è€—æ—¶ï¼Œåªåšä¸€æ¬¡)
            avg_probs = None
            for model in models:
                logits = model(seqs, mask=masks.to(device))
                probs = torch.sigmoid(logits)
                if avg_probs is None:
                    avg_probs = probs
                else:
                    avg_probs += probs
            avg_probs /= len(models)

            probs_np = avg_probs.cpu().numpy()
            labels_np = labels.cpu().numpy()
            masks_np = masks.cpu().numpy()

            # 2. é’ˆå¯¹ä¸åŒé˜ˆå€¼å¾ªç¯è§£ç  (çº¯ CPU è®¡ç®—ï¼Œå¾ˆå¿«)
            batch_size = probs_np.shape[0]
            for k in range(batch_size):
                valid_len = int(masks_np[k].sum())
                prob_map = probs_np[k, :valid_len, :valid_len]
                true_map = labels_np[k, :valid_len, :valid_len]

                for t in thresholds:
                    # ä½¿ç”¨å½“å‰é˜ˆå€¼è§£ç 
                    pred_map = greedy_decoding(prob_map, threshold=t)

                    # è®¡ç®—æŒ‡æ ‡
                    tp = np.sum(pred_map * true_map)
                    fp = np.sum(pred_map) - tp
                    fn = np.sum(true_map) - tp

                    p = tp / (tp + fp + 1e-10)
                    r = tp / (tp + fn + 1e-10)
                    f1 = 2 * p * r / (p + r + 1e-10)

                    metrics[t]['p_sum'] += p
                    metrics[t]['r_sum'] += r
                    metrics[t]['f1_sum'] += f1

                count += 1

    print(f"\nğŸ“Š === æœ€ç»ˆç»“æœæŠ¥å‘Š (æ ·æœ¬æ•°: {count}) ===")

    best_avg_f1 = 0
    best_t = 0

    # æ‰“å°è¡¨å¤´
    print(f"{'Threshold':<10} | {'Precision':<10} | {'Recall':<10} | {'F1 Score':<10}")
    print("-" * 50)

    for t in thresholds:
        avg_p = metrics[t]['p_sum'] / count
        avg_r = metrics[t]['r_sum'] / count
        avg_f1 = metrics[t]['f1_sum'] / count

        print(f"{t:<10.2f} | {avg_p:<10.4f} | {avg_r:<10.4f} | {avg_f1:<10.4f}")

        if avg_f1 > best_avg_f1:
            best_avg_f1 = avg_f1
            best_t = t

    print("-" * 50)
    print(f"ğŸ† æœ€ä½³é˜ˆå€¼: {best_t} | æœ€ä½³ F1: {best_avg_f1:.4f}")


# --- ä¸»å‡½æ•°å…¥å£ ---
if __name__ == "__main__":
    # 1. å®šä¹‰æ¨¡å‹åˆ—è¡¨ (å¡«å…¥ä½ å¾®è°ƒåçš„ Epoch 1, 2, 3 æƒé‡)
    # è¯·åŠ¡å¿…ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„ï¼ï¼ï¼
    checkpoint_paths = [
        r"D:\PycharmProjects\tRNATransferPrediction\checkpoints\model_transformer_epoch_1.pth",  # å¡«ä½ çš„æ–‡ä»¶å
        r"D:\PycharmProjects\tRNATransferPrediction\checkpoints\model_transformer_epoch_2.pth",
        r"D:\PycharmProjects\tRNATransferPrediction\checkpoints\model_transformer_epoch_3.pth"
    ]

    print(f"æ­£åœ¨åŠ è½½ {len(checkpoint_paths)} ä¸ªæ¨¡å‹...")
    models = []
    for path in checkpoint_paths:
        try:
            m = SpotRNA_LSTM_Refined(Config).to(Config.DEVICE)
            # æ³¨æ„ map_locationï¼Œé˜²æ­¢æ˜¾å­˜ä¸è¶³
            m.load_state_dict(torch.load(path, map_location=Config.DEVICE))
            models.append(m)
            print(f"æˆåŠŸåŠ è½½: {path}")
        except Exception as e:
            print(f"åŠ è½½å¤±è´¥ {path}: {e}")

    if not models:
        print("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ¨¡å‹ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
        exit()

    # 2. å‡†å¤‡æ•°æ®é›†
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    dataset = MultiFileDataset(Config.DATA_DIR, max_len=Config.MAX_LEN)
    # batch_size ç¨å¾®å¤§ç‚¹è·‘å¾—å¿«ï¼Œåªè¦ä¸çˆ†æ˜¾å­˜
    dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE * 4, shuffle=False, collate_fn=collate_pad)

    # 3. è¿è¡Œç½‘æ ¼æœç´¢
    evaluate_with_grid_search(models, dataloader, Config.DEVICE)